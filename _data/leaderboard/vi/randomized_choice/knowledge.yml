ViMMRC:
  URA-LLaMa 70B:
    AC: 0.76
    AC_std: 0.02
    F1: 0.76
    F1_std: 0.02
    AR: 0.78
    AR_std: 0.01
    ECE: 0.14
    ECE_std: 0.02
    A@10: 0.94
    A@10_std: 0.04
  URA-LLaMa 13B:
    AC: 0.62
    AC_std: 0.02
    F1: 0.62
    F1_std: 0.02
    AR: 0.61
    AR_std: 0.02
    ECE: 0.15
    ECE_std: 0.02
    A@10: 0.67
    A@10_std: 0.07
  URA-LLaMa 7B:
    AC: 0.45
    AC_std: 0.02
    F1: 0.36
    F1_std: 0.02
    AR: 0.57
    AR_std: 0.02
    ECE: 0.10
    ECE_std: 0.02
    A@10: 0.45
    A@10_std: 0.07
  LLaMa-2 13B:
    AC: 0.57
    AC_std: 0.02
    F1: 0.57
    F1_std: 0.02
    AR: 0.57
    AR_std: 0.02
    ECE: 0.29
    ECE_std: 0.02
    A@10: 0.75
    A@10_std: 0.07
  LLaMa-2 7B:
    AC: 0.36
    AC_std: 0.02
    F1: 0.27
    F1_std: 0.02
    AR: 0.56
    AR_std: 0.02
    ECE: 0.37
    ECE_std: 0.02
    A@10: 0.44
    A@10_std: 0.07
  Vietcuna 7B:
    AC: 0.26
    AC_std: 0.02
    F1: 0.15
    F1_std: 0.01
    AR: 0.50
    AR_std: 0.00
    ECE: 0.01
    ECE_std: 0.01
    A@10: 0.26
    A@10_std: 0.06
  MixSUra:
    AC: 0.61
    AC_std: null
    F1: 0.61
    F1_std: null
    AR: 0.54
    AR_std: null
    ECE: 0.31
    ECE_std: null
    A@10: 0.65
    A@10_std: null
  GPT-3.5:
    AC: 0.92
    AC_std: 0.01
    F1: 0.74
    F1_std: 0.04
    AR: null
    AR_std: null
    ECE: 0.67
    ECE_std: 0.01
    A@10: 0.92
    A@10_std: 0.04
  GPT-4:
    AC: 0.92
    AC_std: 0.01
    F1: 0.74
    F1_std: 0.04
    AR: null
    AR_std: null
    ECE: 0.67
    ECE_std: 0.01
    A@10: 0.92
    A@10_std: 0.04