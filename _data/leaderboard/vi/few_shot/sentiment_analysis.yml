VLSP 2016:
  URA-LLaMa 70B:
    AC: 0.66
    AC_std: 0.01
    F1: 0.49
    F1_std: 0.01
    AR: 0.72
    AR_std: 0.01
    ECE: 0.13
    ECE_std: 0.01
    A@10: 0.77
    A@10_std: 0.04
  URA-LLaMa 13B:
    AC: 0.59
    AC_std: 0.01
    F1: 0.57
    F1_std: 0.01
    AR: 0.67
    AR_std: 0.01
    ECE: 0.09
    ECE_std: 0.01
    A@10: 0.82
    A@10_std: 0.04
  URA-LLaMa 7B:
    AC: 0.57
    AC_std: 0.02
    F1: 0.42
    F1_std: 0.05
    AR: 0.69
    AR_std: 0.02
    ECE: 0.07
    ECE_std: 0.02
    A@10: 0.77
    A@10_std: 0.04
  LLaMa-2 13B:
    AC: 0.51
    AC_std: 0.01
    F1: 0.41
    F1_std: 0.06
    AR: 0.66
    AR_std: 0.01
    ECE: 0.32
    ECE_std: 0.02
    A@10: 0.80
    A@10_std: 0.04
  LLaMa-2 7B:
    AC: 0.45
    AC_std: 0.01
    F1: 0.32
    F1_std: 0.01
    AR: 0.59
    AR_std: 0.01
    ECE: 0.26
    ECE_std: 0.02
    A@10: 0.50
    A@10_std: 0.05
  Vietcuna 7B:
    AC: 0.04
    AC_std: 0.01
    F1: 0.05
    F1_std: 0.01
    AR: 0.45
    AR_std: 0.01
    ECE: 0.71
    ECE_std: 0.01
    A@10: 0.05
    A@10_std: 0.02
  MixSUra:
    AC: 0.62
    AC_std: null
    F1: 0.63
    F1_std: null
    AR: 0.59
    AR_std: null
    ECE: 0.30
    ECE_std: null
    A@10: 0.59
    A@10_std: null
  GPT-3.5:
    AC: 0.65
    AC_std: 0.01
    F1: 0.59
    F1_std: 0.1
    AR: null
    AR_std: null
    ECE: 0.32
    ECE_std: 0.01
    A@10: 0.65
    A@10_std: 0.05
  GPT-4:
    AC: 0.75
    AC_std: 0.01
    F1: 0.74
    F1_std: 0.01
    AR: null
    AR_std: null
    ECE: 0.41
    ECE_std: 0.01
    A@10: 0.74
    A@10_std: 0.04
UiT-VSFC:
  URA-LLaMa 70B:
    AC: 0.75
    AC_std: 0.01
    F1: 0.48
    F1_std: 0.01
    AR: 0.81
    AR_std: 0.01
    ECE: 0.16
    ECE_std: 0.01
    A@10: 0.71
    A@10_std: 0.02
  URA-LLaMa 13B:
    AC: 0.74
    AC_std: 0.01
    F1: 0.52
    F1_std: 0.08
    AR: 0.83
    AR_std: 0.01
    ECE: 0.10
    ECE_std: 0.01
    A@10: 0.87
    A@10_std: 0.02
  URA-LLaMa 7B:
    AC: 0.72
    AC_std: 0.01
    F1: 0.43
    F1_std: 0.01
    AR: 0.78
    AR_std: 0.01
    ECE: 0.13
    ECE_std: 0.01
    A@10: 0.95
    A@10_std: 0.03
  LLaMa-2 13B:
    AC: 0.63
    AC_std: 0.01
    F1: 0.46
    F1_std: 0.07
    AR: 0.71
    AR_std: 0.01
    ECE: 0.13
    ECE_std: 0.01
    A@10: 0.88
    A@10_std: 0.02
  LLaMa-2 7B:
    AC: 0.50
    AC_std: 0.01
    F1: 0.34
    F1_std: 0.01
    AR: 0.69
    AR_std: 0.01
    ECE: 0.23
    ECE_std: 0.01
    A@10: 0.62
    A@10_std: 0.03
  Vietcuna 7B:
    AC: 0.03
    AC_std: 0.00
    F1: 0.03
    F1_std: 0.00
    AR: 0.53
    AR_std: 0.01
    ECE: 0.50
    ECE_std: 0.00
    A@10: 0.01
    A@10_std: 0.00
  MixSUra:
    AC: 0.74
    AC_std: null
    F1: 0.46
    F1_std: null
    AR: 0.63
    AR_std: null
    ECE: 0.23
    ECE_std: null
    A@10: 0.655
    A@10_std: null
  GPT-3.5:
    AC: 0.86
    AC_std: 0.01
    F1: 0.73
    F1_std: 0.01
    AR: null
    AR_std: null
    ECE: 0.52
    ECE_std: 0.01
    A@10: 0.86
    A@10_std: 0.02
  GPT-4:
    AC: 0.85
    AC_std: 0.01
    F1: 0.59
    F1_std: 0.09
    AR: null
    AR_std: null
    ECE: 0.52
    ECE_std: 0.01
    A@10: 0.85
    A@10_std: 0.02