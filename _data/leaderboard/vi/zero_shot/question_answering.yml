XQuAD:
  URA-LLaMa 70B:
    EM: 0.06
    EM_std: 0.00
    F1: 0.30
    F1_std: 0.00
  URA-LLaMa 13B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.14
    F1_std: 0.00
  URA-LLaMa 7B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.14
    F1_std: 0.00
  LLaMa-2 13B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.04
    F1_std: 0.00
  LLaMa-2 7B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.05
    F1_std: 0.00
  Vietcuna 7B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.00
    F1_std: 0.00
  GPT-3.5:
    EM: 0.00
    EM_std: 0.00
    F1: 0.24
    F1_std: 0.00
  GPT-4:
    EM: 0.00
    EM_std: 0.00
    F1: 0.27
    F1_std: 0.00
MLQA:
  URA-LLaMa 70B:
    EM: 0.04
    EM_std: 0.00
    F1: 0.28
    F1_std: 0.00
  URA-LLaMa 13B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.15
    F1_std: 0.00
  URA-LLaMa 7B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.16
    F1_std: 0.00
  LLaMa-2 13B:
    EM: 0.00 
    EM_std: 0.02
    F1: 0.05
    F1_std: 0.00
  LLaMa-2 7B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.06
    F1_std: 0.00
  Vietcuna 7B:
    EM: 0.00
    EM_std: 0.00
    F1: 0.00
    F1_std: 0.00
  GPT-3.5:
    EM: 0.00
    EM_std: 0.00
    F1: 0.25
    F1_std: 0.00
  GPT-4:
    EM: 0.00
    EM_std: 0.00
    F1: 0.27
    F1_std: 0.00