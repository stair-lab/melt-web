UiT-VSMEC:
  URA-LLaMa 70B:
    AC: 0.40
    AC_std: 0.02
    F1: 0.32
    F1_std: 0.02
    AR: 0.68
    AR_std: 0.01
    ECE: 0.14
    ECE_std: 0.02
    A@10: 0.60
    A@10_std: 0.06
  URA-LLaMa 13B:
    AC: 0.29
    AC_std: 0.02
    F1: 0.25
    F1_std: 0.02
    AR: 0.52
    AR_std: 0.01
    ECE: 0.09
    ECE_std: 0.01
    A@10: 0.23
    A@10_std: 0.05
  URA-LLaMa 7B:
    AC: 0.13
    AC_std: 0.01
    F1: 0.11
    F1_std: 0.01
    AR: 0.50
    AR_std: 0.01
    ECE: 0.15
    ECE_std: 0.01
    A@10: 0.21
    A@10_std: 0.05
  LLaMa-2 13B:
    AC: 0.11
    AC_std: 0.01
    F1: 0.10
    F1_std: 0.01
    AR: 0.49
    AR_std: 0.01
    ECE: 0.31
    ECE_std: 0.01
    A@10: 0.09
    A@10_std: 0.04
  LLaMa-2 7B:
    AC: 0.07
    AC_std: 0.01
    F1: 0.08
    F1_std: 0.01
    AR: 0.52
    AR_std: 0.01
    ECE: 0.35
    ECE_std: 0.01
    A@10: 0.07
    A@10_std: 0.03
  Vietcuna 7B:
    AC: 0.05
    AC_std: 0.01
    F1: 0.02
    F1_std: 0.01
    AR: 0.52
    AR_std: 0.01
    ECE: 0.95
    ECE_std: 0.01
    A@10: 0.03
    A@10_std: 0.02
  MixSUra:
    AC: 0.40
    AC_std: null
    F1: 0.36
    F1_std: null
    AR: 0.72
    AR_std: null
    ECE: 0.53
    ECE_std: null
    A@10: 0.79
    A@10_std: null
  GPT-3.5:
    AC: 0.43
    AC_std: 0.02
    F1: 0.37
    F1_std: 0.02
    AR: null
    AR_std: null
    ECE: 0.29
    ECE_std: 0.02
    A@10: 0.43
    A@10_std: 0.06
  GPT-4:
    AC: 0.49
    AC_std: 0.02
    F1: 0.46
    F1_std: 0.02
    AR: null
    AR_std: null
    ECE: 0.35
    ECE_std: 0.02
    A@10: 0.50
    A@10_std: 0.06
PhoATIS:
  URA-LLaMa 70B:
    AC: 0.56
    AC_std: 0.02
    F1: 0.48
    F1_std: 0.03
    AR: 0.85
    AR_std: 0.00
    ECE: 0.25
    ECE_std: 0.02
    A@10: 0.56
    A@10_std: 0.06
  URA-LLaMa 13B:
    AC: 0.10
    AC_std: 0.01
    F1: 0.10
    F1_std: 0.01
    AR: 0.72
    AR_std: 0.00
    ECE: 0.52
    ECE_std: 0.01
    A@10: 0.14
    A@10_std: 0.04
  URA-LLaMa 7B:
    AC: 0.04
    AC_std: 0.01
    F1: 0.04
    F1_std: 0.02
    AR: 0.77
    AR_std: 0.00
    ECE: 0.30
    ECE_std: 0.01
    A@10: 0.04
    A@10_std: 0.02
  LLaMa-2 13B:
    AC: 0.03
    AC_std: 0.01
    F1: 0.02
    F1_std: 0.00
    AR: 0.45
    AR_std: 0.01
    ECE: 0.28
    ECE_std: 0.01
    A@10: 0.03
    A@10_std: 0.02
  LLaMa-2 7B:
    AC: 0.00
    AC_std: 0.06
    F1: 0.00
    F1_std: 0.06
    AR: 0.61
    AR_std: 0.01
    ECE: 0.32
    ECE_std: 0.00
    A@10: 0.00
    A@10_std: 0.00
  Vietcuna 7B:
    AC: 0.05
    AC_std: 0.01
    F1: 0.01
    F1_std: 0.00
    AR: 0.66
    AR_std: 0.00
    ECE: 0.20
    ECE_std: 0.01
    A@10: 0.01
    A@10_std: 0.21
  MixSUra:
    AC: 0.81
    AC_std: null
    F1: 0.58
    F1_std: null
    AR: 0.96
    AR_std: null
    ECE: 0.14
    ECE_std: null
    A@10: 0.91
    A@10_std: null
  GPT-3.5:
    AC: 0.44
    AC_std: 0.02
    F1: 0.38
    F1_std: 0.03
    AR: null
    AR_std: null
    ECE: 0.38
    ECE_std: 0.02
    A@10: 0.44
    A@10_std: 0.05
  GPT-4:
    AC: 0.89
    AC_std: 0.01
    F1: 0.69
    F1_std: 0.02
    AR: null
    AR_std: null
    ECE: 0.83
    ECE_std: 0.01
    A@10: 0.89
    A@10_std: 0.03