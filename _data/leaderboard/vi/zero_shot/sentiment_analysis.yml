VLSP 2016:
  URA-LLaMa 70B:
    AC: 0.63
    AC_std: 0.02
    F1: 0.63
    F1_std: 0.02
    AR: 0.74
    AR_std: 0.01
    ECE: 0.15
    ECE_std: 0.01
    A@10: 0.87
    A@10_std: 0.03
  URA-LLaMa 13B:
    AC: 0.52
    AC_std: 0.02
    F1: 0.35
    F1_std: 0.01
    AR: 0.60
    AR_std: 0.01
    ECE: 0.10
    ECE_std: 0.01
    A@10: 0.64
    A@10_std: 0.05
  URA-LLaMa 7B:
    AC: 0.35
    AC_std: 0.02
    F1: 0.24
    F1_std: 0.01
    AR: 0.54
    AR_std: 0.01
    ECE: 0.24
    ECE_std: 0.01
    A@10: 0.31
    A@10_std: 0.05
  LLaMa-2 13B:
    AC: 0.25
    AC_std: 0.01
    F1: 0.25
    F1_std: 0.01
    AR: 0.49
    AR_std: 0.01
    ECE: 0.39
    ECE_std: 0.01
    A@10: 0.29
    A@10_std: 0.05
  LLaMa-2 7B:
    AC: 0.15
    AC_std: 0.01
    F1: 0.15
    F1_std: 0.01
    AR: 0.58
    AR_std: 0.01
    ECE: 0.73
    ECE_std: 0.01
    A@10: 0.12
    A@10_std: 0.03
  Vietcuna 7B:
    AC: 0.11
    AC_std: 0.01
    F1: 0.12
    F1_std: 0.01
    AR: 0.49
    AR_std: 0.01
    ECE: 0.68
    ECE_std: 0.01
    A@10: 0.11
    A@10_std: 0.03
  MixSUra:
    AC: 0.45
    AC_std: null
    F1: 0.30
    F1_std: null
    AR: 0.62
    AR_std: null
    ECE: 0.50
    ECE_std: null
    A@10: 0.49
    A@10_std: null
  GPT-3.5:
    AC: 0.62
    AC_std: 0.02
    F1: 0.56
    F1_std: 0.01
    AR: null
    AR_std: null
    ECE: 0.29
    ECE_std: 0.02
    A@10: 0.62
    A@10_std: 0.05
  GPT-4:
    AC: 0.71
    AC_std: 0.01
    F1: 0.68
    F1_std: 0.01
    AR: null
    AR_std: null
    ECE: 0.37
    ECE_std: 0.01
    A@10: 0.70
    A@10_std: 0.04
UiT-VSFC:
  URA-LLaMa 70B:
    AC: 0.64
    AC_std: 0.01
    F1: 0.54
    F1_std: 0.01
    AR: 0.85
    AR_std: 0.01
    ECE: 0.14
    ECE_std: 0.00
    A@10: 0.98
    A@10_std: 0.01
  URA-LLaMa 13B:
    AC: 0.70
    AC_std: 0.01
    F1: 0.40
    F1_std: 0.01
    AR: 0.72
    AR_std: 0.01
    ECE: 0.23
    ECE_std: 0.01
    A@10: 0.95
    A@10_std: 0.01
  URA-LLaMa 7B:
    AC: 0.27
    AC_std: 0.01
    F1: 0.18
    F1_std: 0.00
    AR: 0.52
    AR_std: 0.01
    ECE: 0.37
    ECE_std: 0.01
    A@10: 0.03
    A@10_std: 0.01
  LLaMa-2 13B:
    AC: 0.29
    AC_std: 0.01
    F1: 0.24
    F1_std: 0.01
    AR: 0.52
    AR_std: 0.01
    ECE: 0.42
    ECE_std: 0.01
    A@10: 0.30
    A@10_std: 0.03
  LLaMa-2 7B:
    AC: 0.04
    AC_std: 0.00
    F1: 0.06
    F1_std: 0.01
    AR: 0.49
    AR_std: 0.01
    ECE: 0.79
    ECE_std: 0.00
    A@10: 0.01
    A@10_std: 0.01
  Vietcuna 7B:
    AC: 0.05
    AC_std: 0.00
    F1: 0.06
    F1_std: 0.00
    AR: 0.56
    AR_std: 0.01
    ECE: 0.73
    ECE_std: 0.00
    A@10: 0.05
    A@10_std: 0.01
  MixSUra:
    AC: 0.55
    AC_std: null
    F1: 0.40
    F1_std: null
    AR: 0.66
    AR_std: null
    ECE: 0.41
    ECE_std: null
    A@10: 0.60
    A@10_std: null
  GPT-3.5:
    AC: 0.86
    AC_std: 0.01
    F1: 0.71
    F1_std: 0.01
    AR: null
    AR_std: null
    ECE: 0.52
    ECE_std: 0.01
    A@10: 0.86
    A@10_std: 0.02
  GPT-4:
    AC: 0.85
    AC_std: 0.01
    F1: 0.71
    F1_std: 0.01
    AR: null
    AR_std: null
    ECE: 0.52
    ECE_std: 0.01
    A@10: 0.87
    A@10_std: 0.02