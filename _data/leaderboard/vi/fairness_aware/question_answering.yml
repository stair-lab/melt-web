XQuAD:
    URA-LLaMa 70B:
      EM: 0.04
      EM_std: 0.00
      F1: 0.27
      F1_std: 0.00
    URA-LLaMa 13B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.13
      F1_std: 0.00
    URA-LLaMa 7B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.13
      F1_std: 0.00
    LLaMa-2 13B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.03
      F1_std: 0.00
    LLaMa-2 7B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.04
      F1_std: 0.00
    Vietcuna 7B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.00
      F1_std: 0.00
    GPT-3.5:
      EM: 0.00
      EM_std: 0.00
      F1: 0.24
      F1_std: 0.00
    GPT-4:
      EM: 0.00
      EM_std: 0.00
      F1: 0.26
      F1_std: 0.00
MLQA:
    URA-LLaMa 70B:
      EM: 0.03
      EM_std: 0.00
      F1: 0.25
      F1_std: 0.00
    URA-LLaMa 13B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.14
      F1_std: 0.00
    URA-LLaMa 7B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.15
      F1_std: 0.01
    LLaMa-2 13B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.04
      F1_std: 0.00
    LLaMa-2 7B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.05
      F1_std: 0.00
    Vietcuna 7B:
      EM: 0.00
      EM_std: 0.00
      F1: 0.00
      F1_std: 0.00
    GPT-3.5:
      EM: 0.00
      EM_std: 0.00
      F1: 0.23
      F1_std: 0.00
    GPT-4:
      EM: 0.00
      EM_std: 0.00
      F1: 0.24
      F1_std: 0.00