VLSP 2016:
    URA-LLaMa 70B:
      AC: 0.63
      AC_std: 0.01
      F1: 0.48
      F1_std: 0.01
      AR: 0.60
      AR_std: 0.01
      ECE: 0.09
      ECE_std: 0.01
      A@10: 0.83
      A@10_std: 0.04
    URA-LLaMa 13B:
      AC: 0.55
      AC_std: 0.02
      F1: 0.52
      F1_std: 0.02
      AR: 0.59
      AR_std: 0.01
      ECE: 0.06
      ECE_std: 0.01
      A@10: 0.74
      A@10_std: 0.05
    URA-LLaMa 7B:
      AC: 0.52
      AC_std: 0.02
      F1: 0.36
      F1_std: 0.03
      AR: 0.59
      AR_std: 0.01
      ECE: 0.07
      ECE_std: 0.01
      A@10: 0.66
      A@10_std: 0.05
    LLaMa-2 13B:
      AC: 0.46
      AC_std: 0.02
      F1: 0.30
      F1_std: 0.01
      AR: 0.55
      AR_std: 0.01
      ECE: 0.39
      ECE_std: 0.02
      A@10: 0.70
      A@10_std: 0.05
    LLaMa-2 7B:
      AC: 0.45
      AC_std: 0.02
      F1: 0.36
      F1_std: 0.01
      AR: 0.54
      AR_std: 0.01
      ECE: 0.20
      ECE_std: 0.02
      A@10: 0.51
      A@10_std: 0.05
    Vietcuna 7B:
      AC: 0.44
      AC_std: 0.02
      F1: 0.27
      F1_std: 0.01
      AR: 0.51
      AR_std: 0.01
      ECE: 0.23
      ECE_std: 0.02
      A@10: 0.53
      A@10_std: 0.05
    MixSUra:
      AC: 0.59
      AC_std: null
      F1: 0.59
      F1_std: null
      AR: 0.55
      AR_std: null
      ECE: 0.34
      ECE_std: null
      A@10: 0.52
      A@10_std: null
    GPT-3.5:
      AC: 0.64
      AC_std: 0.01
      F1: 0.60
      F1_std: 0.01
      AR: null
      AR_std: null
      ECE: 0.31
      ECE_std: 0.01
      A@10: 0.54
      A@10_std: 0.05
    GPT-4:
      AC: 0.74
      AC_std: 0.00
      F1: 0.73
      F1_std: 0.00
      AR: null
      AR_std: null
      ECE: 0.41
      ECE_std: 0.00
      A@10: 0.71
      A@10_std: 0.00
UiT-VSFC:
    URA-LLaMa 70B:
      AC: 0.71
      AC_std: 0.01
      F1: 0.45
      F1_std: 0.01
      AR: 0.80
      AR_std: 0.01
      ECE: 0.08
      ECE_std: 0.01
      A@10: 0.99
      A@10_std: 0.01
    URA-LLaMa 13B:
      AC: 0.72
      AC_std: 0.01
      F1: 0.44
      F1_std: 0.05
      AR: 0.77
      AR_std: 0.01
      ECE: 0.18
      ECE_std: 0.01
      A@10: 0.77
      A@10_std: 0.02
    URA-LLaMa 7B:
      AC: 0.73
      AC_std: 0.01
      F1: 0.41
      F1_std: 0.01
      AR: 0.71
      AR_std: 0.01
      ECE: 0.16
      ECE_std: 0.01
      A@10: 0.87
      A@10_std: 0.02
    LLaMa-2 13B:
      AC: 0.66
      AC_std: 0.01
      F1: 0.40
      F1_std: 0.01
      AR: 0.63
      AR_std: 0.01
      ECE: 0.11
      ECE_std: 0.01
      A@10: 0.89
      A@10_std: 0.02
    LLaMa-2 7B:
      AC: 0.51
      AC_std: 0.01
      F1: 0.33
      F1_std: 0.01
      AR: 0.65
      AR_std: 0.01
      ECE: 0.15
      ECE_std: 0.01
      A@10: 0.80
      A@10_std: 0.02
    Vietcuna 7B:
      AC: 0.49
      AC_std: 0.01
      F1: 0.25
      F1_std: 0.03
      AR: 0.46
      AR_std: 0.01
      ECE: 0.33
      ECE_std: 0.01
      A@10: 0.34
      A@10_std: 0.03
    MixSUra:
      AC: 0.69
      AC_std: null
      F1: 0.44
      F1_std: null
      AR: 0.61
      AR_std: null
      ECE: 0.29
      ECE_std: null
      A@10: 0.66
      A@10_std: null
    GPT-3.5:
      AC: 0.86
      AC_std: 0.01
      F1: 0.71
      F1_std: 0.01
      AR: null
      AR_std: null
      ECE: 0.53
      ECE_std: 0.01
      A@10: 0.86
      A@10_std: 0.02
    GPT-4:
      AC: 0.83
      AC_std: 0.00
      F1: 0.70
      F1_std: 0.00
      AR: null
      AR_std: null
      ECE: 0.50
      ECE_std: 0.00
      A@10: 0.85
      A@10_std: 0.00